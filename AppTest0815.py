# -*- coding: utf-8 -*-
"""「修改後RAGIIM」的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjvpOow5lAEwmQWt6YBdH6_sw055PLSs
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q llama-index google-generativeai
# %pip install llama-index-llms-gemini
# %pip install llama-index-embeddings-huggingface
#%pip install llama-index-readers-wikipedia
#!pip install wikipedia
!pip install -q llama-index google-generativeai pandas llama-index-llms-gemini llama-index-embeddings-huggingface
!pip list | grep llama

#解釋這個code以及使用技術
import os
import pandas as pd
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, Document
from llama_index.core import VectorStoreIndex, get_response_synthesizer
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

GOOGLE_API_KEY = "AIzaSyDHdddKwG41Ig3p5bVfIUQ2w-X6bOZI3gk"  #input API key
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# HuggingFace 嵌入模型，將文本轉成向量
#embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-zh-v1.5")
csv_file_path = 'output1.csv'
data = pd.read_csv(csv_file_path) #讀文件儲存於變量
print(data)

documents = []
skipNUM = 0
for index, row in data.iterrows():
    if pd.isna(row['title']): #跳過title空的行
        print(f"Skipping row {index-skipNUM} due to empty 'title' field")
        skipNUM += 1
        continue
    if pd.notna(row['title']):
        content = (f"title: {row['title']}\n" f"detail: {row['detail']}")

        #print(f"Content for row {index-skipNUM}: {content}")
        #改!
        documents.append(Document(id=str(index-skipNUM),text=content))# 索引作为ID       # 將detail的東西存入
    else:
        print(f"Skipping row {index-skipNUM} due to missing data in other fields")

if not documents:
    raise ValueError("No valid documents found. Ensure your CSV file has content.")

#print(documents)
# 檢索文檔: 使用嵌入模型將文件轉成向量
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
###############


# configure response synthesizer
response_synthesizer = get_response_synthesizer(response_mode="tree_summarize",llm=None)
# assemble query engine
query_engine = RetrieverQueryEngine(retriever=retriever,response_synthesizer=response_synthesizer,llm=Gemini())


#################
# 將轉成的向量丟入查詢引擎
query_engine = index.as_query_engine(llm=Gemini())

# 执行查询
#query = "Introduce Wang, I-Lin's. (繁體中文回答大於五十字)，並解釋簡述研究興趣的一般學術定義"  # 示例查询
query="資管營?"
response = query_engine.query(query)

# 打印结果
print("Response:")
print(response)

from llama_index.core import Settings
Settings.llm = Gemini()
# configure retriever
retriever = VectorIndexRetriever(index=index,similarity_top_k=3,)
response_synthesizer = get_response_synthesizer(response_mode="generation",llm=None)
# assemble query engine
query_engine = RetrieverQueryEngine(retriever=retriever,response_synthesizer=response_synthesizer)


#################
# 將轉成的向量丟入查詢引擎
query_engine = index.as_query_engine(llm=Gemini())

# 执行查询
#query = "Introduce Wang, I-Lin's. (繁體中文回答大於五十字)，並解釋簡述研究興趣的一般學術定義"  # 示例查询
query="資管營?(繁體中文回答)"
response = query_engine.query(query)

# 打印结果
print("Response:")
print(response)

#這個無法解決計算的問題、有時候超級不準的

import requests
from bs4 import BeautifulSoup
import csv

# 定义提取函数
def extract_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')

    data = []

    # 查找所有的内容区域
    content_sections = soup.find_all('div', class_='title_name')
    for section in content_sections:
        # 提取标题
        title = section.get_text(strip=True)

        # 获取标题后的内容
        content = []
        next_sibling = section.find_next_sibling()
        while next_sibling and next_sibling.name != 'div':
            if next_sibling.name == 'p':
                content.append(next_sibling.get_text(strip=True))
            elif next_sibling.name == 'ul':
                list_items = next_sibling.find_all('li')
                for item in list_items:
                    content.append(f"• {item.get_text(strip=True)}")
            elif next_sibling.name == 'ol':
                list_items = next_sibling.find_all('li')
                for item in list_items:
                    content.append(f"1. {item.get_text(strip=True)}")
            elif next_sibling.name == 'table':
                table = next_sibling
                table_data = []
                for row in table.find_all('tr'):
                    cells = row.find_all('td')
                    row_data = [cell.get_text(strip=True) for cell in cells]
                    table_data.append(' | '.join(row_data))
                if table_data:
                    content.append(' '.join(table_data))
            next_sibling = next_sibling.find_next_sibling()

        # 拼接内容
        content_text = ' '.join(content)
        data.append({'title': title, 'detail': content_text})

    return data

# URLs to scrape
urls = [
   'https://im.ncku.edu.tw/p/412-1138-20232.php?Lang=zh-tw',
    'https://im.ncku.edu.tw/p/412-1138-19386.php?Lang=zh-tw',
    'https://im.ncku.edu.tw/p/412-1138-20364.php?Lang=zh-tw',
    'https://im.ncku.edu.tw/p/412-1138-20448.php?Lang=zh-tw',
    'https://iim.ncku.edu.tw/p/412-1138-24109.php?Lang=zh-tw',
    'https://im.ncku.edu.tw/p/412-1138-20239.php?Lang=zh-tw',
    'https://im.ncku.edu.tw/p/412-1138-20286.php?Lang=zh-tw',
   'https://im.ncku.edu.tw/p/412-1138-19455.php?Lang=zh-tw',
   'https://im.ncku.edu.tw/p/412-1138-19398.php?Lang=zh-tw',
   'https://im.ncku.edu.tw/p/412-1138-19406.php?Lang=zh-tw',
   'https://im.ncku.edu.tw/p/412-1138-19407.php?Lang=zh-tw'
]

# Collect data from both URLs
all_data = []
for url in urls:
    all_data.extend(extract_data(url))

# Save to CSV file
csv_file = 'output.csv'
csv_columns = ['title', 'detail']

with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.DictWriter(file, fieldnames=csv_columns)
    writer.writeheader()
    for row in all_data:
        writer.writerow(row)

print(f"数据已成功保存到 {csv_file}")

"""測試性能、優化速度、介面、歷史紀錄、溫度、數據優化

"""